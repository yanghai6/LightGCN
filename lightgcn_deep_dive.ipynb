{"cells":[{"cell_type":"markdown","metadata":{"id":"UKIrCdYn2ULI"},"source":["<i>Copyright (c) Recommenders contributors.</i>\n","\n","<i>Licensed under the MIT License.</i>"]},{"cell_type":"markdown","metadata":{"id":"kIMnabv22ULL"},"source":["# LightGCN - simplified GCN model for recommendation\n","\n","This notebook serves as an introduction to LightGCN [1], which is an simple, linear and neat Graph Convolution Network (GCN) [3] model for recommendation."]},{"cell_type":"markdown","metadata":{"id":"rBnLesoU2ULM"},"source":["## 0 Global Settings and Imports"]},{"cell_type":"code","source":["# In order to make things work on google drive\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive', force_remount=True)\n","# !ls gdrive/MyDrive/Colab\\ Notebooks/LightGCN-PyTorch-master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ksPgImLh2h0u","executionInfo":{"status":"ok","timestamp":1702414261915,"user_tz":300,"elapsed":13478,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"outputId":"732a1959-9a01-4bb8-8abb-1ebeac64d57a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir('/content/gdrive/MyDrive/Colab Notebooks/LightGCN')"],"metadata":{"id":"uhrhnq_N25PA","executionInfo":{"status":"ok","timestamp":1702414262276,"user_tz":300,"elapsed":3,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install scrapbook"],"metadata":{"id":"6Me7mY9Q2wYq","executionInfo":{"status":"ok","timestamp":1702414268929,"user_tz":300,"elapsed":6655,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"03187b59-55d3-4a35-ec10-80ad6c1f6fdd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scrapbook\n","  Downloading scrapbook-0.5.0-py3-none-any.whl (34 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from scrapbook) (1.5.3)\n","Collecting papermill (from scrapbook)\n","  Downloading papermill-2.5.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from scrapbook) (4.19.2)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from scrapbook) (7.34.0)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from scrapbook) (10.0.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (67.7.2)\n","Collecting jedi>=0.16 (from ipython->scrapbook)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (3.0.41)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (4.9.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (2023.11.2)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (0.31.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (0.13.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scrapbook) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scrapbook) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->scrapbook) (1.23.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (8.1.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (6.0.1)\n","Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (5.9.2)\n","Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (0.9.0)\n","Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (2.31.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (0.4)\n","Requirement already satisfied: tenacity>=5.0.2 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (8.2.3)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->scrapbook) (0.8.3)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (6.1.12)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (5.5.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (2.19.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->scrapbook) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->scrapbook) (0.2.12)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->scrapbook) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (2023.11.17)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill->scrapbook) (23.2.1)\n","Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill->scrapbook) (6.3.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill->scrapbook) (4.1.0)\n","Installing collected packages: jedi, papermill, scrapbook\n","Successfully installed jedi-0.19.1 papermill-2.5.0 scrapbook-0.5.0\n"]}]},{"cell_type":"code","source":["!pip install retrying"],"metadata":{"id":"E5hoWWai3gO6","executionInfo":{"status":"ok","timestamp":1702414275458,"user_tz":300,"elapsed":6532,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6808f04-fa8a-44b4-9995-682fe42501b1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting retrying\n","  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying) (1.16.0)\n","Installing collected packages: retrying\n","Successfully installed retrying-1.3.4\n"]}]},{"cell_type":"code","source":["!pip install pandera"],"metadata":{"id":"o4hRClfJ3l1v","executionInfo":{"status":"ok","timestamp":1702414286850,"user_tz":300,"elapsed":11235,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5134e3c8-ee08-4bd3-e7d4-bdb94a201797"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pandera\n","  Downloading pandera-0.18.0-py3-none-any.whl (209 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multimethod (from pandera)\n","  Downloading multimethod-1.10-py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from pandera) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pandera) (23.2)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pandera) (1.5.3)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from pandera) (1.10.13)\n","Collecting typeguard>=3.0.2 (from pandera)\n","  Downloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n","Collecting typing-inspect>=0.6.0 (from pandera)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from pandera) (1.14.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pandera) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pandera) (2023.3.post1)\n","Collecting typing-extensions>=4.7.0 (from typeguard>=3.0.2->pandera)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.6.0->pandera)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pandera) (1.16.0)\n","Installing collected packages: typing-extensions, mypy-extensions, multimethod, typing-inspect, typeguard, pandera\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed multimethod-1.10 mypy-extensions-1.0.0 pandera-0.18.0 typeguard-4.1.5 typing-extensions-4.9.0 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["!cat recommenders/models/deeprec/DataModel/ImplicitCF.py"],"metadata":{"id":"UnEH8NCOHo_D","executionInfo":{"status":"ok","timestamp":1702410811679,"user_tz":300,"elapsed":29,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"44273e0f-e21e-4358-835c-396262639ac0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# Copyright (c) Recommenders contributors.\n","# Licensed under the MIT License.\n","\n","import random\n","import numpy as np\n","import pandas as pd\n","import scipy.sparse as sp\n","\n","from recommenders.utils.constants import (\n","    DEFAULT_ITEM_COL,\n","    DEFAULT_USER_COL,\n","    DEFAULT_RATING_COL,\n","    DEFAULT_PREDICTION_COL,\n",")\n","\n","\n","class ImplicitCF(object):\n","    \"\"\"Data processing class for GCN models which use implicit feedback.\n","\n","    Initialize train and test set, create normalized adjacency matrix and sample data for training epochs.\n","\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        train,\n","        test=None,\n","        adj_dir=None,\n","        col_user=DEFAULT_USER_COL,\n","        col_item=DEFAULT_ITEM_COL,\n","        col_rating=DEFAULT_RATING_COL,\n","        col_prediction=DEFAULT_PREDICTION_COL,\n","        seed=None,\n","    ):\n","        \"\"\"Constructor\n","\n","        Args:\n","            adj_dir (str): Directory to save / load adjacency matrices. If it is None, adjacency\n","                matrices will be created and will not be saved.\n","            train (pandas.DataFrame): Training data with at least columns (col_user, col_item, col_rating).\n","            test (pandas.DataFrame): Test data with at least columns (col_user, col_item, col_rating).\n","                test can be None, if so, we only process the training data.\n","            col_user (str): User column name.\n","            col_item (str): Item column name.\n","            col_rating (str): Rating column name.\n","            seed (int): Seed.\n","\n","        \"\"\"\n","        self.user_idx = None\n","        self.item_idx = None\n","        self.adj_dir = adj_dir\n","        self.col_user = col_user\n","        self.col_item = col_item\n","        self.col_rating = col_rating\n","        self.col_prediction = col_prediction\n","        self.train, self.test = self._data_processing(train, test)\n","        self._init_train_data()\n","\n","        random.seed(seed)\n","\n","    def _data_processing(self, train, test):\n","        \"\"\"Process the dataset to reindex userID and itemID and only keep records with ratings greater than 0.\n","\n","        Args:\n","            train (pandas.DataFrame): Training data with at least columns (col_user, col_item, col_rating).\n","            test (pandas.DataFrame): Test data with at least columns (col_user, col_item, col_rating).\n","                test can be None, if so, we only process the training data.\n","\n","        Returns:\n","            list: train and test pandas.DataFrame Dataset, which have been reindexed and filtered.\n","\n","        \"\"\"\n","        df = (\n","            train\n","            if test is None\n","            else pd.concat([train, test], axis=0, ignore_index=True)\n","        )\n","\n","        if self.user_idx is None:\n","            user_idx = df[[self.col_user]].drop_duplicates().reindex()\n","            user_idx[self.col_user + \"_idx\"] = np.arange(len(user_idx))\n","            self.n_users = len(user_idx)\n","            self.user_idx = user_idx\n","\n","            self.user2id = dict(\n","                zip(user_idx[self.col_user], user_idx[self.col_user + \"_idx\"])\n","            )\n","            self.id2user = dict(\n","                zip(user_idx[self.col_user + \"_idx\"], user_idx[self.col_user])\n","            )\n","\n","        if self.item_idx is None:\n","            item_idx = df[[self.col_item]].drop_duplicates()\n","            item_idx[self.col_item + \"_idx\"] = np.arange(len(item_idx))\n","            self.n_items = len(item_idx)\n","            self.item_idx = item_idx\n","\n","            self.item2id = dict(\n","                zip(item_idx[self.col_item], item_idx[self.col_item + \"_idx\"])\n","            )\n","            self.id2item = dict(\n","                zip(item_idx[self.col_item + \"_idx\"], item_idx[self.col_item])\n","            )\n","\n","        return self._reindex(train), self._reindex(test)\n","\n","    def _reindex(self, df):\n","        \"\"\"Process the dataset to reindex userID and itemID and only keep records with ratings greater than 0.\n","\n","        Args:\n","            df (pandas.DataFrame): dataframe with at least columns (col_user, col_item, col_rating).\n","\n","        Returns:\n","            list: train and test pandas.DataFrame Dataset, which have been reindexed and filtered.\n","\n","        \"\"\"\n","\n","        if df is None:\n","            return None\n","\n","        df = pd.merge(df, self.user_idx, on=self.col_user, how=\"left\")\n","        df = pd.merge(df, self.item_idx, on=self.col_item, how=\"left\")\n","\n","        df = df[df[self.col_rating] > 0]\n","\n","        df_reindex = df[\n","            [self.col_user + \"_idx\", self.col_item + \"_idx\", self.col_rating]\n","        ]\n","        df_reindex.columns = [self.col_user, self.col_item, self.col_rating]\n","\n","        return df_reindex\n","\n","    def _init_train_data(self):\n","        \"\"\"Record items interated with each user in a dataframe self.interact_status, and create adjacency\n","        matrix self.R.\n","\n","        \"\"\"\n","        self.interact_status = (\n","            self.train.groupby(self.col_user)[self.col_item]\n","            .apply(set)\n","            .reset_index()\n","            .rename(columns={self.col_item: self.col_item + \"_interacted\"})\n","        )\n","        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n","        self.R[self.train[self.col_user], self.train[self.col_item]] = 1.0\n","\n","    def get_norm_adj_mat(self):\n","        \"\"\"Load normalized adjacency matrix if it exists, otherwise create (and save) it.\n","\n","        Returns:\n","            scipy.sparse.csr_matrix: Normalized adjacency matrix.\n","\n","        \"\"\"\n","        try:\n","            if self.adj_dir is None:\n","                raise FileNotFoundError\n","            norm_adj_mat = sp.load_npz(self.adj_dir + \"/norm_adj_mat.npz\")\n","            print(\"Already load norm adj matrix.\")\n","\n","        except FileNotFoundError:\n","            norm_adj_mat = self.create_norm_adj_mat()\n","            if self.adj_dir is not None:\n","                sp.save_npz(self.adj_dir + \"/norm_adj_mat.npz\", norm_adj_mat)\n","        return norm_adj_mat\n","\n","    def create_norm_adj_mat(self):\n","        \"\"\"Create normalized adjacency matrix.\n","\n","        Returns:\n","            scipy.sparse.csr_matrix: Normalized adjacency matrix.\n","\n","        \"\"\"\n","        adj_mat = sp.dok_matrix(\n","            (self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32\n","        )\n","        adj_mat = adj_mat.tolil()\n","        R = self.R.tolil()\n","\n","        adj_mat[: self.n_users, self.n_users :] = R\n","        adj_mat[self.n_users :, : self.n_users] = R.T\n","        adj_mat = adj_mat.todok()\n","        print(\"Already create adjacency matrix.\")\n","\n","        rowsum = np.array(adj_mat.sum(1))\n","        d_inv = np.power(rowsum + 1e-9, -0.5).flatten()\n","        d_inv[np.isinf(d_inv)] = 0.0\n","        d_mat_inv = sp.diags(d_inv)\n","        norm_adj_mat = d_mat_inv.dot(adj_mat)\n","        norm_adj_mat = norm_adj_mat.dot(d_mat_inv)\n","        print(\"Already normalize adjacency matrix.\")\n","\n","        return norm_adj_mat.tocsr()\n","\n","    def train_loader(self, batch_size):\n","        \"\"\"Sample train data every batch. One positive item and one negative item sampled for each user.\n","\n","        Args:\n","            batch_size (int): Batch size of users.\n","\n","        Returns:\n","            numpy.ndarray, numpy.ndarray, numpy.ndarray:\n","            - Sampled users.\n","            - Sampled positive items.\n","            - Sampled negative items.\n","        \"\"\"\n","\n","        def sample_neg(x):\n","            while True:\n","                neg_id = random.randint(0, self.n_items - 1)\n","                if neg_id not in x:\n","                    return neg_id\n","\n","        indices = range(self.n_users)\n","        if self.n_users < batch_size:\n","            users = [random.choice(indices) for _ in range(batch_size)]\n","        else:\n","            users = random.sample(indices, batch_size)\n","\n","        interact = self.interact_status.iloc[users]\n","        pos_items = interact[self.col_item + \"_interacted\"].apply(\n","            lambda x: random.choice(list(x))\n","        )\n","        neg_items = interact[self.col_item + \"_interacted\"].apply(\n","            lambda x: sample_neg(x)\n","        )\n","\n","        return np.array(users), np.array(pos_items), np.array(neg_items)\n"]}]},{"cell_type":"code","source":["!cat recommenders/models/deeprec/models/graphrec/lightgcn.py"],"metadata":{"id":"8xfun6sBYUo0","executionInfo":{"status":"ok","timestamp":1702410812027,"user_tz":300,"elapsed":373,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eeaa20c6-975d-42db-9ed7-efa87ddf953a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# Copyright (c) Recommenders contributors.\n","# Licensed under the MIT License.\n","\n","import tensorflow as tf\n","import time\n","import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","import random\n","from recommenders.evaluation.python_evaluation import (\n","    map_at_k,\n","    ndcg_at_k,\n","    precision_at_k,\n","    recall_at_k,\n",")\n","from recommenders.utils.python_utils import get_top_k_scored_items\n","\n","tf.compat.v1.disable_eager_execution ()  # need to disable eager in TF2.x\n","\n","\n","class LightGCN (object):\n","    \"\"\"LightGCN model\n","\n","    :Citation:\n","\n","        He, Xiangnan, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang.\n","        \"LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation.\" arXiv\n","        preprint arXiv:2002.02126, 2020.\n","    \"\"\"\n","\n","    def __init__(self, hparams, data, seed=None):\n","        \"\"\"Initializing the model. Create parameters, placeholders, embeddings and loss function.\n","\n","        Args:\n","            hparams (HParams): A HParams object, hold the entire set of hyperparameters.\n","            data (object): A recommenders.models.deeprec.DataModel.ImplicitCF object, load and process data.\n","            seed (int): Seed.\n","\n","        \"\"\"\n","\n","        tf.compat.v1.set_random_seed (seed)\n","        np.random.seed (seed)\n","\n","        self.data = data\n","        self.epochs = hparams.epochs\n","        self.lr = hparams.learning_rate\n","        self.emb_dim = hparams.embed_size\n","        self.batch_size = hparams.batch_size\n","        self.n_layers = hparams.n_layers\n","        self.decay = hparams.decay\n","        self.eval_epoch = hparams.eval_epoch\n","        self.top_k = hparams.top_k\n","        self.save_model = hparams.save_model\n","        self.save_epoch = hparams.save_epoch\n","        self.metrics = hparams.metrics\n","        self.model_dir = hparams.MODEL_DIR\n","\n","        metric_options = [\"map\", \"ndcg\", \"precision\", \"recall\"]\n","        for metric in self.metrics:\n","            if metric not in metric_options:\n","                raise ValueError (\n","                    \"Wrong metric(s), please select one of this list: {}\".format (\n","                        metric_options\n","                    )\n","                )\n","\n","        self.norm_adj = data.get_norm_adj_mat ()\n","\n","        self.n_users = data.n_users\n","        self.n_items = data.n_items\n","\n","        self.users = tf.compat.v1.placeholder (tf.int32, shape=(None,))\n","        self.pos_items = tf.compat.v1.placeholder (tf.int32, shape=(None,))\n","        self.neg_items = tf.compat.v1.placeholder (tf.int32, shape=(None,))\n","\n","        self.weights = self._init_weights ()\n","        self.ua_embeddings, self.ia_embeddings = self._create_lightgcn_embed ()\n","\n","        self.u_g_embeddings = tf.nn.embedding_lookup (\n","            params=self.ua_embeddings, ids=self.users\n","        )\n","        self.pos_i_g_embeddings = tf.nn.embedding_lookup (\n","            params=self.ia_embeddings, ids=self.pos_items\n","        )\n","        self.neg_i_g_embeddings = tf.nn.embedding_lookup (\n","            params=self.ia_embeddings, ids=self.neg_items\n","        )\n","        self.u_g_embeddings_pre = tf.nn.embedding_lookup (\n","            params=self.weights[\"user_embedding\"], ids=self.users\n","        )\n","        self.pos_i_g_embeddings_pre = tf.nn.embedding_lookup (\n","            params=self.weights[\"item_embedding\"], ids=self.pos_items\n","        )\n","        self.neg_i_g_embeddings_pre = tf.nn.embedding_lookup (\n","            params=self.weights[\"item_embedding\"], ids=self.neg_items\n","        )\n","\n","        self.batch_ratings = tf.matmul (\n","            self.u_g_embeddings,\n","            self.pos_i_g_embeddings,\n","            transpose_a=False,\n","            transpose_b=True,\n","        )\n","\n","        self.mf_loss, self.emb_loss = self._create_bpr_loss (\n","            self.u_g_embeddings, self.pos_i_g_embeddings, self.neg_i_g_embeddings\n","        )\n","        self.loss = self.mf_loss + self.emb_loss\n","\n","        self.opt = tf.compat.v1.train.AdamOptimizer (learning_rate=self.lr).minimize (\n","            self.loss\n","        )\n","        self.saver = tf.compat.v1.train.Saver (max_to_keep=1)\n","\n","        gpu_options = tf.compat.v1.GPUOptions (allow_growth=True)\n","        self.sess = tf.compat.v1.Session (\n","            config=tf.compat.v1.ConfigProto (gpu_options=gpu_options)\n","        )\n","        self.sess.run (tf.compat.v1.global_variables_initializer ())\n","\n","    def _init_weights(self):\n","        \"\"\"Initialize user and item embeddings.\n","\n","        Returns:\n","            dict: With keys `user_embedding` and `item_embedding`, embeddings of all users and items.\n","\n","        \"\"\"\n","        all_weights = dict ()\n","        initializer = tf.compat.v1.keras.initializers.VarianceScaling (\n","            scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"\n","        )\n","\n","        all_weights[\"user_embedding\"] = tf.Variable (\n","            initializer ([self.n_users, self.emb_dim]), name=\"user_embedding\"\n","        )\n","        all_weights[\"item_embedding\"] = tf.Variable (\n","            initializer ([self.n_items, self.emb_dim]), name=\"item_embedding\"\n","        )\n","        print (\"Using xavier initialization.\")\n","\n","        return all_weights\n","\n","    def _create_lightgcn_embed(self):\n","        \"\"\"Calculate the average embeddings of users and items after every layer of the model.\n","\n","        Returns:\n","            tf.Tensor, tf.Tensor: Average user embeddings. Average item embeddings.\n","\n","        \"\"\"\n","        A_hat = self._convert_sp_mat_to_sp_tensor (self.norm_adj)\n","\n","        ego_embeddings = tf.concat (\n","            [self.weights[\"user_embedding\"], self.weights[\"item_embedding\"]], axis=0\n","        )\n","        all_embeddings = [ego_embeddings]\n","\n","        for k in range (0, self.n_layers):\n","            ego_embeddings = tf.sparse.sparse_dense_matmul (A_hat, ego_embeddings)\n","            all_embeddings += [ego_embeddings]\n","\n","        all_embeddings = tf.stack (all_embeddings, 1)\n","        all_embeddings = tf.reduce_mean (\n","            input_tensor=all_embeddings, axis=1, keepdims=False\n","        )\n","        u_g_embeddings, i_g_embeddings = tf.split (\n","            all_embeddings, [self.n_users, self.n_items], 0\n","        )\n","        return u_g_embeddings, i_g_embeddings\n","\n","    def _create_bpr_loss(self, users, pos_items, neg_items):\n","        \"\"\"Calculate BPR loss.\n","\n","        Args:\n","            users (tf.Tensor): User embeddings to calculate loss.\n","            pos_items (tf.Tensor): Positive item embeddings to calculate loss.\n","            neg_items (tf.Tensor): Negative item embeddings to calculate loss.\n","\n","        Returns:\n","            tf.Tensor, tf.Tensor: Matrix factorization loss. Embedding regularization loss.\n","\n","        \"\"\"\n","        pos_scores = tf.reduce_sum (input_tensor=tf.multiply (users, pos_items), axis=1)\n","        neg_scores = tf.reduce_sum (input_tensor=tf.multiply (users, neg_items), axis=1)\n","\n","        regularizer = (\n","                tf.nn.l2_loss (self.u_g_embeddings_pre)\n","                + tf.nn.l2_loss (self.pos_i_g_embeddings_pre)\n","                + tf.nn.l2_loss (self.neg_i_g_embeddings_pre)\n","        )\n","        regularizer = regularizer / self.batch_size\n","        mf_loss = tf.reduce_mean (\n","            input_tensor=tf.nn.softplus (-(pos_scores - neg_scores))\n","        )\n","        emb_loss = self.decay * regularizer\n","        return mf_loss, emb_loss\n","\n","    def _convert_sp_mat_to_sp_tensor(self, X):\n","        \"\"\"Convert a scipy sparse matrix to tf.SparseTensor.\n","\n","        Returns:\n","            tf.SparseTensor: SparseTensor after conversion.\n","\n","        \"\"\"\n","        coo = X.tocoo ().astype (np.float32)\n","        indices = np.mat ([coo.row, coo.col]).transpose ()\n","        return tf.SparseTensor (indices, coo.data, coo.shape)\n","\n","    def fit(self, neg_mode=\"uniform\", neg_size = 0):\n","        \"\"\"Fit the model on self.data.train. If eval_epoch is not -1, evaluate the model on `self.data.test`\n","        every `eval_epoch` epoch to observe the training status.\n","\n","        \"\"\"\n","        mask = None\n","        all_users = np.array(range(self.n_users))\n","        interact = self.data.interact_status.iloc[all_users]\n","        for epoch in range (1, self.epochs + 1):\n","            train_start = time.time ()\n","            loss, mf_loss, emb_loss = 0.0, 0.0, 0.0\n","\n","            hard_users = []\n","            hard_pos = []\n","            hard_neg = []\n","\n","            if neg_mode ==\"epoch\" and epoch > 1:\n","                scores = self.score(all_users)\n","                if mask is None:\n","                    mask = np.zeros_like(scores, dtype = bool)\n","                    for user_idx in all_users:\n","                        mask[user_idx] = np.isin(range(self.n_items), interact)\n","                scores[mask] = 0\n","                # for i in range(scores.shape[0]):\n","                #     for j in range(scores.shape[1]):\n","                #         if j in interact[i]:\n","                #             scores[i, j] = 0\n","                indices = np.array(np.argsort(scores.ravel()))[0][-neg_size::]\n","                for idx in indices:\n","                    uid = idx // scores.shape[1]\n","                    iid = idx % scores.shape[1]\n","                    hard_users.append(uid)\n","                    hard_neg.append(iid)\n","                    hard_pos.append(random.choice(list(interact[self.data.col_item + \"_interacted\"][uid])))\n","\n","            n_batch = self.data.train.shape[0] // self.batch_size + 1\n","            for idx in range (n_batch):\n","                users, pos_items, neg_items = self.data.train_loader (self.batch_size)\n","                if neg_mode == \"uniform\":\n","                    f_users, f_pos_items, f_neg_items = users, pos_items, neg_items\n","                if neg_mode == \"hard\" or \"epoch\" or \"hard2\":\n","                    f_users = np.append (users, hard_users)\n","                    f_pos_items = np.append (pos_items, hard_pos)\n","                    f_neg_items = np.append (neg_items, hard_neg)\n","                _, batch_loss, batch_mf_loss, batch_emb_loss = self.sess.run (\n","                    [self.opt, self.loss, self.mf_loss, self.emb_loss],\n","                    feed_dict={\n","                        self.users: f_users,\n","                        self.pos_items: f_pos_items,\n","                        self.neg_items: f_neg_items,\n","                    },\n","                )\n","                loss += batch_loss / n_batch\n","                mf_loss += batch_mf_loss / n_batch\n","                emb_loss += batch_emb_loss / n_batch\n","\n","                if neg_mode == \"hard2\":\n","                    scores = self.score(users, remove_seen=False)\n","                    interact = self.data.interact_status.iloc[users]\n","                    neg_scores = []\n","                    neg = []\n","                    pos = []\n","                    for i, u in enumerate(users):\n","                        positive = list(interact[self.data.col_item + \"_interacted\"][u])\n","                        user_scores = scores[i]\n","                        mask = ~np.isin(np.arange (len (user_scores)), positive)\n","                        masked_scores = user_scores[mask]\n","                        s = np.max(masked_scores)\n","                        neg_scores.append(s)\n","                        neg.append(int(np.where(user_scores == s)[0]))\n","                        pos.append(random.choice(positive))\n","                    idx = np.argsort (neg_scores)[-neg_size:]\n","                    hard_users = users[idx]\n","                    hard_pos = pos[idx]\n","                    hard_neg = neg[idx]\n","\n","                if neg_mode == \"hard\":\n","                    # scores = self.score(users, remove_seen=False)\n","                    # interact = self.data.interact_status.iloc[users]\n","                    # neg_scores = []\n","                    # neg = []\n","                    # pos = []\n","                    # for i, u in enumerate(users):\n","                    #     positive = interact[self.data.col_item + \"_interacted\"][u]\n","                    #     user_scores = scores[i]\n","                    #     mask = ~np.isin(np.arange (len (user_scores)), positive)\n","                    #     masked_scores = user_scores[mask]\n","                    #     s = np.max(masked_scores)\n","                    #     neg_scores.append(s)\n","                    #     neg.append(int(np.where(user_scores == s)[0]))\n","                    #     pos.append(random.choice(list(positive)))\n","                    # idx = np.argsort (neg_scores)[-neg_size:]\n","                    # hard_users = users[idx]\n","                    # hard_pos = pos[idx]\n","                    # hard_neg = neg[idx]\n","\n","                    scores = self.score (users, remove_seen=False)\n","                    neg_scores = [scores[i][neg_items[i]] for i in range (len (users))]\n","                    idx = np.argsort(neg_scores)[-neg_size:]\n","                    # print(idx)\n","                    hard_users = users[idx]\n","                    hard_pos = pos_items[idx]\n","                    hard_neg = neg_items[idx]\n","\n","            if np.isnan (loss):\n","                print (\"ERROR: loss is nan.\")\n","                sys.exit ()\n","            train_end = time.time ()\n","            train_time = train_end - train_start\n","\n","            if self.save_model and epoch % self.save_epoch == 0:\n","                save_path_str = os.path.join (self.model_dir, \"epoch_\" + str (epoch))\n","                if not os.path.exists (save_path_str):\n","                    os.makedirs (save_path_str)\n","                checkpoint_path = self.saver.save (  # noqa: F841\n","                    sess=self.sess, save_path=save_path_str\n","                )\n","                print (\"Save model to path {0}\".format (os.path.abspath (save_path_str)))\n","\n","            if self.eval_epoch == -1 or epoch % self.eval_epoch != 0:\n","                print (\n","                    \"Epoch %d (train)%.1fs: train loss = %.5f = (mf)%.5f + (embed)%.5f\"\n","                    % (epoch, train_time, loss, mf_loss, emb_loss)\n","                )\n","            else:\n","                eval_start = time.time ()\n","                ret = self.run_eval ()\n","                eval_end = time.time ()\n","                eval_time = eval_end - eval_start\n","\n","                print (\n","                    \"Epoch %d (train)%.1fs + (eval)%.1fs: train loss = %.5f = (mf)%.5f + (embed)%.5f, %s\"\n","                    % (\n","                        epoch,\n","                        train_time,\n","                        eval_time,\n","                        loss,\n","                        mf_loss,\n","                        emb_loss,\n","                        \", \".join (\n","                            metric + \" = %.5f\" % (r)\n","                            for metric, r in zip (self.metrics, ret)\n","                        ),\n","                    )\n","                )\n","\n","\n","    #     def fit(self, neg_mode=\"uniform\"):\n","    #         \"\"\"Fit the model on self.data.train. If eval_epoch is not -1, evaluate the model on `self.data.test`\n","    #         every `eval_epoch` epoch to observe the training status.\n","    #\n","    #         \"\"\"\n","    #         '''\n","    #\n","    #     def sample_hard_neg(x):\n","    #         pos_items = set (x)\n","    #\n","    #         best_score = float ('-inf')\n","    #         num_samples = 10\n","    #\n","    #         sampled_ids = np.random.choice (a=self.n_items, size=num_samples, replace=False)\n","    #         scores = self.score (sampled_ids)\n","    #\n","    #         non_pos_mask = np.array ([item not in pos_items for item in sampled_ids])\n","    #\n","    #         filtered_ids = sampled_ids[non_pos_mask]\n","    #         filtered_scores = scores[non_pos_mask]\n","    #\n","    #         if len (filtered_scores) > 0:\n","    #             max_score_idx = np.argmax (filtered_scores)\n","    #             best_neg_id = filtered_ids[max_score_idx]\n","    #         else:\n","    #             best_neg_id = None\n","    #\n","    #         return best_neg_id\n","    #\n","    #     '''\n","    #    def sample_hard_neg(x):\n","    #        pos_items = set(x)\n","    #\n","    #        best_score = float('-inf')\n","    #        num_samples = 10\n","    #\n","    #        sampled_ids = np.random.choice(a=self.n_users-1, size=num_samples, replace=False)\n","    #        scores = self.score(sampled_ids, remove_seen=False)\n","    #\n","    #\n","    #        non_pos_mask = np.array([item not in pos_items for item in sampled_ids])\n","    #        '''\n","    #     non_pos_mask = ~np.isin (sampled_ids, list (pos_items))\n","    #     '''\n","    #\n","    #     filtered_ids = sampled_ids[non_pos_mask]\n","    #     filtered_scores = scores[non_pos_mask]\n","    #\n","    #     if len(filtered_scores) > 0:\n","    #         #filtered_scores = np.sum(filtered_scores, axis=1) # sum over rows\n","    #         #indices = np.argmax(filtered_scores)\n","    #         #best_neg_id = filtered_ids[indices]\n","    #         indices = np.where(filtered_scores == filtered_scores.max())\n","    #         best_neg_id = indices[0]\n","    #         best_neg_id = filtered_ids[indices[0]]\n","    #         '''\n","    #     flat_index = np.argmax (filtered_scores.ravel ())\n","    #     best_neg_id = flat_index // filtered_scores.shape[1]\n","    #     '''\n","    # else:\n","    #     #all positive\n","    #\n","    #     while True:\n","    #         neg_id = random.randint(0, self.n_users - 1)\n","    #         if neg_id not in x:\n","    #             return neg_id\n","    #     '''\n","    #     neg_candidates = np.setdiff1d (np.arange (self.n_users - 1), list (pos_items), assume_unique=True)\n","    #     best_neg_id = np.random.choice (neg_candidates, 1)[0]\n","    #     '''\n","    #\n","    # return best_neg_id\n","    #\n","    #\n","    # '''\n","    #\n","    #\n","    # for _ in range (num_samples):\n","    #     neg_id = random.randint (0, self.n_items - 1)\n","    #     best_neg_id = neg_id\n","    #     if neg_id not in pos_items:\n","    #         # Check if the model predicts a higher score for the negative item\n","    #         try:\n","    #             # Check if the model predicts a higher score for the negative item\n","    #             score = self.score (np.array ([neg_id]))\n","    #\n","    #             if np.max (score) > best_score:\n","    #                 best_score = np.max (score)\n","    #                 best_neg_id = neg_id\n","    #         except Exception as e:\n","    #             continue\n","    # return best_neg_id\n","    # '''\n","    #\n","    #\n","    # for epoch in range(1, self.epochs + 1):\n","    # train_start = time.time()\n","    # loss, mf_loss, emb_loss = 0.0, 0.0, 0.0\n","    # n_batch = self.data.train.shape[0] // self.batch_size + 1\n","    # for idx in range(n_batch):\n","    #     users, pos_items, neg_items = self.data.train_loader(self.batch_size)\n","    #     #print(neg_items)\n","    #     if neg_mode == \"hard\":\n","    #         #print(\"hard\")\n","    #         interact = self.data.interact_status.iloc[users]\n","    #         neg_items = interact[self.data.col_item + \"_interacted\"].apply(lambda x: sample_hard_neg(x))\n","    #         #print(neg_items)\n","    #     elif neg_mode == \"uniform\":\n","    #         pass\n","    #         #print(\"uniform\")\n","    #     _, batch_loss, batch_mf_loss, batch_emb_loss = self.sess.run(\n","    #         [self.opt, self.loss, self.mf_loss, self.emb_loss],\n","    #         feed_dict={\n","    #             self.users: users,\n","    #             self.pos_items: pos_items,\n","    #             self.neg_items: neg_items,\n","    #         },\n","    #     )\n","    #     loss += batch_loss / n_batch\n","    #     mf_loss += batch_mf_loss / n_batch\n","    #     emb_loss += batch_emb_loss / n_batch\n","    #\n","    # if np.isnan(loss):\n","    #     print(\"ERROR: loss is nan.\")\n","    #     sys.exit()\n","    # train_end = time.time()\n","    # train_time = train_end - train_start\n","    #\n","    # if self.save_model and epoch % self.save_epoch == 0:\n","    #     save_path_str = os.path.join(self.model_dir, \"epoch_\" + str(epoch))\n","    #     if not os.path.exists(save_path_str):\n","    #         os.makedirs(save_path_str)\n","    #     checkpoint_path = self.saver.save(  # noqa: F841\n","    #         sess=self.sess, save_path=save_path_str\n","    #     )\n","    #     print(\"Save model to path {0}\".format(os.path.abspath(save_path_str)))\n","    #\n","    # if self.eval_epoch == -1 or epoch % self.eval_epoch != 0:\n","    #     print(\n","    #         \"Epoch %d (train)%.1fs: train loss = %.5f = (mf)%.5f + (embed)%.5f\"\n","    #         % (epoch, train_time, loss, mf_loss, emb_loss)\n","    #     )\n","    # else:\n","    #     eval_start = time.time()\n","    #     ret = self.run_eval()\n","    #     eval_end = time.time()\n","    #     eval_time = eval_end - eval_start\n","    #\n","    #     print(\n","    #         \"Epoch %d (train)%.1fs + (eval)%.1fs: train loss = %.5f = (mf)%.5f + (embed)%.5f, %s\"\n","    #         % (\n","    #             epoch,\n","    #             train_time,\n","    #             eval_time,\n","    #             loss,\n","    #             mf_loss,\n","    #             emb_loss,\n","    #             \", \".join(\n","    #                 metric + \" = %.5f\" % (r)\n","    #                 for metric, r in zip(self.metrics, ret)\n","    #             ),\n","    #         )\n","    #     )\n","\n","\n","    def load(self, model_path=None):\n","        \"\"\"Load an existing model.\n","\n","        Args:\n","            model_path: Model path.\n","\n","        Raises:\n","            IOError: if the restore operation failed.\n","\n","        \"\"\"\n","        try:\n","            self.saver.restore (self.sess, model_path)\n","        except Exception:\n","            raise IOError (\n","                \"Failed to find any matching files for {0}\".format (model_path)\n","            )\n","\n","\n","    def run_eval(self):\n","        \"\"\"Run evaluation on self.data.test.\n","\n","        Returns:\n","            dict: Results of all metrics in `self.metrics`.\n","        \"\"\"\n","        topk_scores = self.recommend_k_items (\n","            self.data.test, top_k=self.top_k, use_id=True\n","        )\n","        ret = []\n","        for metric in self.metrics:\n","            if metric == \"map\":\n","                ret.append (map_at_k (self.data.test, topk_scores, k=self.top_k))\n","            elif metric == \"ndcg\":\n","                ret.append (ndcg_at_k (self.data.test, topk_scores, k=self.top_k))\n","            elif metric == \"precision\":\n","                ret.append (precision_at_k (self.data.test, topk_scores, k=self.top_k))\n","            elif metric == \"recall\":\n","                ret.append (recall_at_k (self.data.test, topk_scores, k=self.top_k))\n","        return ret\n","\n","\n","    def score(self, user_ids, remove_seen=True):\n","        \"\"\"Score all items for test users.\n","\n","        Args:\n","            user_ids (np.array): Users to test.\n","            remove_seen (bool): Flag to remove items seen in training from recommendation.\n","\n","        Returns:\n","            numpy.ndarray: Value of interest of all items for the users.\n","\n","        \"\"\"\n","        if any (np.isnan (user_ids)):\n","            raise ValueError (\n","                \"LightGCN cannot score users that are not in the training set\"\n","            )\n","        u_batch_size = self.batch_size\n","        n_user_batchs = len (user_ids) // u_batch_size + 1\n","        test_scores = []\n","        for u_batch_id in range (n_user_batchs):\n","            start = u_batch_id * u_batch_size\n","            end = (u_batch_id + 1) * u_batch_size\n","            user_batch = user_ids[start:end]\n","            item_batch = range (self.data.n_items)\n","            rate_batch = self.sess.run (\n","                self.batch_ratings, {self.users: user_batch, self.pos_items: item_batch}\n","            )\n","            test_scores.append (np.array (rate_batch))\n","        test_scores = np.concatenate (test_scores, axis=0)\n","        if remove_seen:\n","            test_scores += self.data.R.tocsr ()[user_ids, :] * -np.inf\n","        return test_scores\n","\n","\n","    def recommend_k_items(\n","            self, test, top_k=10, sort_top_k=True, remove_seen=True, use_id=False\n","    ):\n","        \"\"\"Recommend top K items for all users in the test set.\n","\n","        Args:\n","            test (pandas.DataFrame): Test data.\n","            top_k (int): Number of top items to recommend.\n","            sort_top_k (bool): Flag to sort top k results.\n","            remove_seen (bool): Flag to remove items seen in training from recommendation.\n","\n","        Returns:\n","            pandas.DataFrame: Top k recommendation items for each user.\n","\n","        \"\"\"\n","        data = self.data\n","        if not use_id:\n","            user_ids = np.array ([data.user2id[x] for x in test[data.col_user].unique ()])\n","        else:\n","            user_ids = np.array (test[data.col_user].unique ())\n","\n","        test_scores = self.score (user_ids, remove_seen=remove_seen)\n","\n","        top_items, top_scores = get_top_k_scored_items (\n","            scores=test_scores, top_k=top_k, sort_top_k=sort_top_k\n","        )\n","\n","        df = pd.DataFrame (\n","            {\n","                data.col_user: np.repeat (\n","                    test[data.col_user].drop_duplicates ().values, top_items.shape[1]\n","                ),\n","                data.col_item: top_items.flatten ()\n","                if use_id\n","                else [data.id2item[item] for item in top_items.flatten ()],\n","                data.col_prediction: top_scores.flatten (),\n","            }\n","        )\n","\n","        return df.replace (-np.inf, np.nan).dropna ()\n","\n","\n","    def output_embeddings(self, idmapper, n, target, user_file):\n","        embeddings = list (target.eval (session=self.sess))\n","        with open (user_file, \"w\") as wt:\n","            for i in range (n):\n","                wt.write (\n","                    \"{0}\\t{1}\\n\".format (\n","                        idmapper[i], \" \".join ([str (a) for a in embeddings[i]])\n","                    )\n","                )\n","\n","\n","    def infer_embedding(self, user_file, item_file):\n","        \"\"\"Export user and item embeddings to csv files.\n","\n","        Args:\n","            user_file (str): Path of file to save user embeddings.\n","            item_file (str): Path of file to save item embeddings.\n","\n","        \"\"\"\n","        # create output directories if they do not exist\n","        dirs, _ = os.path.split (user_file)\n","        if not os.path.exists (dirs):\n","            os.makedirs (dirs)\n","        dirs, _ = os.path.split (item_file)\n","        if not os.path.exists (dirs):\n","            os.makedirs (dirs)\n","\n","        data = self.data\n","\n","        self.output_embeddings (\n","            data.id2user, self.n_users, self.ua_embeddings, user_file\n","        )\n","        self.output_embeddings (\n","            data.id2item, self.n_items, self.ia_embeddings, item_file\n","        )\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"nFNecv7j2ULM","executionInfo":{"status":"ok","timestamp":1702414300891,"user_tz":300,"elapsed":14044,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"745be476-88cf-4255-ba04-08cb229955eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["System version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n","Pandas version: 1.5.3\n","Tensorflow version: 2.14.0\n"]}],"source":["import sys\n","import os\n","import scrapbook as sb\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","tf.get_logger().setLevel('ERROR') # only show error messages\n","\n","from recommenders.utils.timer import Timer\n","from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN\n","from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n","from recommenders.datasets import movielens\n","from recommenders.datasets.python_splitters import python_stratified_split\n","from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n","from recommenders.utils.constants import SEED as DEFAULT_SEED\n","from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n","\n","print(\"System version: {}\".format(sys.version))\n","print(\"Pandas version: {}\".format(pd.__version__))\n","print(\"Tensorflow version: {}\".format(tf.__version__))"]},{"cell_type":"code","execution_count":7,"metadata":{"tags":["parameters"],"id":"VM2Y6wGx2ULO","executionInfo":{"status":"ok","timestamp":1702414300891,"user_tz":300,"elapsed":14,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}}},"outputs":[],"source":["# top k items to recommend\n","TOP_K = 10\n","\n","# Select MovieLens data size: 100k, 1m, 10m, or 20m\n","MOVIELENS_DATA_SIZE = '100k'\n","\n","# Model parameters\n","EPOCHS = 50\n","BATCH_SIZE = 1024\n","\n","SEED = DEFAULT_SEED  # Set None for non-deterministic results\n","\n","yaml_file = \"recommenders/models/deeprec/config/lightgcn.yaml\"\n","user_file = \"tests/resources/deeprec/lightgcn/user_embeddings.csv\"\n","item_file = \"tests/resources/deeprec/lightgcn/item_embeddings.csv\""]},{"cell_type":"markdown","metadata":{"id":"j3-nm2Eg2ULP"},"source":["## 1 LightGCN model\n","\n","LightGCN is a simplified version of Neural Graph Collaborative Filtering (NGCF) [4], which adapts GCNs in recommendation systems.\n","\n","### 1.1 Graph Networks in Recommendation Systems\n","\n","GCN are networks that can learn patterns in graph data. They can be applied in many fields, but they are particularly well suited for Recommendation Systems, because of their ability to encode relationships.\n","\n","In traditional models like matrix factorization [5], user and items are represented as embeddings. And the interaction, which is the signal that encodes the behavior, is not part of the embeddings, but it is represented in the loss function, typically as a dot product.\n","\n","Despite their effectiveness, some authors [1,4] argue that these methods are not sufficient to yield satisfactory embeddings for collaborative filtering. The key reason is that the embedding function lacks an explicit encoding of the crucial collaborative signal, which is latent in user-item interactions to reveal the behavioral similarity between users (or items).\n","\n","**GCNs can be used to encode the interaction signal in the embeddings**. Interacted items can be seen as user´s features, because they provide direct evidence on a user’s preference. Similarly, the users that consume an item can be treated as the item’s features and used to measure the collaborative similarity of two items. A natural way to incorporate the interaction signal in the embedding is by exploiting the high-order connectivity from user-item interactions.\n","\n","In the figure below, the user-item interaction is shown (to the left) as well as the concept of higher-order connectivity (to the right).\n","\n","<img src=\"https://recodatasets.z20.web.core.windows.net/images/High_order_connectivity.png\" width=500 style=\"display:block; margin-left:auto; margin-right:auto;\">\n","\n","The high-order connectivity shows the collaborative signal in a graph form. For example, the path $u_1 ← i_2 ← u2$ indicates the behavior\n","similarity between $u_1$ and $u_2$, as both users have interacted with $i_2$; the longer path $u_1 ← i_2 ← u_2 ← i_4$ suggests that $u_1$ is likely to adopt $i_4$, since her similar user $u_2$ has consumed $i_4$ before. Moreover, from the holistic view of $l = 3$, item $i_4$ is more likely to be of interest to $u_1$ than item $i_5$, since there are two paths connecting $(i_4,u_1)$, while only one path connects $(i_5,u_1)$.\n","\n","Based on this high-order connectivity, NGCF [4] defines an embedding propagation layer, which refines a user’s (or an item’s) embedding by aggregating the embeddings of the interacted items (or users). By stacking multiple embedding propagation layers, we can enforce the embeddings\n","to capture the collaborative signal in high-order connectivities.\n","\n","More formally, let $\\mathbf{e}_{u}^{0}$ denote the original embedding of user $u$ and $\\mathbf{e}_{i}^{0}$ denote the original embedding of item $i$. The embedding propagation can be computed recursively as:\n","\n","$$\n","\\begin{array}{l}\n","\\mathbf{e}_{u}^{(k+1)}=\\sigma\\bigl( \\mathbf{W}_{1}\\mathbf{e}_{u}^{(k)} + \\sum_{i \\in \\mathcal{N}_{u}} \\frac{1}{\\sqrt{\\left|\\mathcal{N}_{u}\\right|} \\sqrt{\\left|\\mathcal{N}_{i}\\right|}} (\\mathbf{W}_{1}\\mathbf{e}_{i}^{(k)} + \\mathbf{W}_{2}(\\mathbf{e}_{i}^{(k)}\\cdot\\mathbf{e}_{u}^{(k)}) ) \\bigr)\n","\\\\\n","\\mathbf{e}_{i}^{(k+1)}=\\sigma\\bigl( \\mathbf{W}_{1}\\mathbf{e}_{i}^{(k)} +\\sum_{u \\in \\mathcal{N}_{i}} \\frac{1}{\\sqrt{\\left|\\mathcal{N}_{i}\\right|} \\sqrt{\\left|\\mathcal{N}_{u}\\right|}} (\\mathbf{W}_{1}\\mathbf{e}_{u}^{(k)} + \\mathbf{W}_{2}(\\mathbf{e}_{u}^{(k)}\\cdot\\mathbf{e}_{i}^{(k)}) ) \\bigr)\n","\\end{array}\n","$$\n","\n","where $\\mathbf{W}_{1}$ and $\\mathbf{W}_{2}$ are trainable weight matrices, $\\frac{1}{\\sqrt{\\left|\\mathcal{N}_{i}\\right|} \\sqrt{\\left|\\mathcal{N}_{u}\\right|}}$ is a discount factor expressed as the graph Laplacian norm, $\\mathcal{N}_{u}$ and $\\mathcal{N}_{i}$ denote the first-hop neighbors of user $u$ and item $i$, and $\\sigma$ is a non-linearity that in the paper is set as a LeakyReLU.\n","\n","To obtain the final representation, each propagated embedding is concatenated (i.e., $\\mathbf{e}_{u}^{(*)}=\\mathbf{e}_{u}^{(0)}||...||\\mathbf{e}_{u}^{(l)}$), and then the final user's preference over an item is computed as a dot product: $\\hat y_{u i} = \\mathbf{e}_{u}^{(*)T}\\mathbf{e}_{i}^{(*)}$.\n","\n","### 1.2 LightGCN architecture\n","\n","LightGCN is a simplified version of NGCF [4] to make it more concise and appropriate for recommendations. The model architecture is illustrated below.\n","\n","<img src=\"https://recodatasets.z20.web.core.windows.net/images/lightGCN-model.jpg\" width=600 style=\"display:block; margin-left:auto; margin-right:auto;\">\n","\n","In Light Graph Convolution, only the normalized sum of neighbor embeddings is performed towards next layer; other operations like self-connection, feature transformation via weight matrices, and nonlinear activation are all removed, which largely simplifies NGCF. In the layer combination step, instead of concatenating the embeddings, we sum over the embeddings at each layer to obtain the final representations.\n","\n","### 1.3 Light Graph Convolution (LGC)\n","\n","In LightGCN, we adopt the simple weighted sum aggregator and abandon the use of feature transformation and nonlinear activation. The graph convolution operation in LightGCN is defined as:\n","\n","$$\n","\\begin{array}{l}\n","\\mathbf{e}_{u}^{(k+1)}=\\sum_{i \\in \\mathcal{N}_{u}} \\frac{1}{\\sqrt{\\left|\\mathcal{N}_{u}\\right|} \\sqrt{\\left|\\mathcal{N}_{i}\\right|}} \\mathbf{e}_{i}^{(k)} \\\\\n","\\mathbf{e}_{i}^{(k+1)}=\\sum_{u \\in \\mathcal{N}_{i}} \\frac{1}{\\sqrt{\\left|\\mathcal{N}_{i}\\right|} \\sqrt{\\left|\\mathcal{N}_{u}\\right|}} \\mathbf{e}_{u}^{(k)}\n","\\end{array}\n","$$\n","\n","The symmetric normalization term $\\frac{1}{\\sqrt{\\left|\\mathcal{N}_{u}\\right|} \\sqrt{\\left|\\mathcal{N}_{i}\\right|}}$ follows the design of standard GCN, which can avoid the scale of embeddings increasing with graph convolution operations.\n","\n","\n","### 1.4 Layer Combination and Model Prediction\n","\n","In LightGCN, the only trainable model parameters are the embeddings at the 0-th layer, i.e., $\\mathbf{e}_{u}^{(0)}$ for all users and $\\mathbf{e}_{i}^{(0)}$ for all items. When they are given, the embeddings at higher layers can be computed via LGC. After $K$ layers LGC, we further combine the embeddings obtained at each layer to form the final representation of a user (an item):\n","\n","$$\n","\\mathbf{e}_{u}=\\sum_{k=0}^{K} \\alpha_{k} \\mathbf{e}_{u}^{(k)} ; \\quad \\mathbf{e}_{i}=\\sum_{k=0}^{K} \\alpha_{k} \\mathbf{e}_{i}^{(k)}\n","$$\n","\n","where $\\alpha_{k} \\geq 0$ denotes the importance of the $k$-th layer embedding in constituting the final embedding. In our experiments, we set $\\alpha_{k}$ uniformly as $1 / (K+1)$.\n","\n","The model prediction is defined as the inner product of user and item final representations:\n","\n","$$\n","\\hat{y}_{u i}=\\mathbf{e}_{u}^{T} \\mathbf{e}_{i}\n","$$\n","\n","which is used as the ranking score for recommendation generation.\n","\n","\n","### 1.5 Matrix Form\n","\n","Let the user-item interaction matrix be $\\mathbf{R} \\in \\mathbb{R}^{M \\times N}$ where $M$ and $N$ denote the number of users and items, respectively, and each entry $R_{ui}$ is 1 if $u$ has interacted with item $i$ otherwise 0. We then obtain the adjacency matrix of the user-item graph as\n","\n","$$\n","\\mathbf{A}=\\left(\\begin{array}{cc}\n","\\mathbf{0} & \\mathbf{R} \\\\\n","\\mathbf{R}^{T} & \\mathbf{0}\n","\\end{array}\\right)\n","$$\n","\n","Let the 0-th layer embedding matrix be $\\mathbf{E}^{(0)} \\in \\mathbb{R}^{(M+N) \\times T}$, where $T$ is the embedding size. Then we can obtain the matrix equivalent form of LGC as:\n","\n","$$\n","\\mathbf{E}^{(k+1)}=\\left(\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}\\right) \\mathbf{E}^{(k)}\n","$$\n","\n","where $\\mathbf{D}$ is a $(M+N) \\times(M+N)$ diagonal matrix, in which each entry $D_{ii}$ denotes the number of nonzero entries in the $i$-th row vector of the adjacency matrix $\\mathbf{A}$ (also named as degree matrix). Lastly, we get the final embedding matrix used for model prediction as:\n","\n","$$\n","\\begin{aligned}\n","\\mathbf{E} &=\\alpha_{0} \\mathbf{E}^{(0)}+\\alpha_{1} \\mathbf{E}^{(1)}+\\alpha_{2} \\mathbf{E}^{(2)}+\\ldots+\\alpha_{K} \\mathbf{E}^{(K)} \\\\\n","&=\\alpha_{0} \\mathbf{E}^{(0)}+\\alpha_{1} \\tilde{\\mathbf{A}} \\mathbf{E}^{(0)}+\\alpha_{2} \\tilde{\\mathbf{A}}^{2} \\mathbf{E}^{(0)}+\\ldots+\\alpha_{K} \\tilde{\\mathbf{A}}^{K} \\mathbf{E}^{(0)}\n","\\end{aligned}\n","$$\n","\n","where $\\tilde{\\mathbf{A}}=\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}$ is the symmetrically normalized matrix.\n","\n","### 1.6 Model Training\n","\n","We employ the Bayesian Personalized Ranking (BPR) loss which is a pairwise loss that encourages the prediction of an observed entry to be higher than its unobserved counterparts:\n","\n","$$\n","L_{B P R}=-\\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_{u}} \\sum_{j \\notin \\mathcal{N}_{u}} \\ln \\sigma\\left(\\hat{y}_{u i}-\\hat{y}_{u j}\\right)+\\lambda\\left\\|\\mathbf{E}^{(0)}\\right\\|^{2}\n","$$\n","\n","Where $\\lambda$ controls the $L_2$ regularization strength. We employ the Adam optimizer and use it in a mini-batch manner.\n"]},{"cell_type":"markdown","metadata":{"id":"nBS7ZvZt2ULQ"},"source":["## 2 TensorFlow implementation of LightGCN with MovieLens dataset\n","\n","We will use the MovieLens dataset, which is composed of integer ratings from 1 to 5.\n","\n","We convert MovieLens into implicit feedback for model training and evaluation.\n","\n","### 2.1 Load and split data\n","\n","We split the full dataset into a `train` and `test` dataset to evaluate performance of the algorithm against a held-out set not seen during training. Because SAR generates recommendations based on user preferences, all users that are in the test set must also exist in the training set. For this case, we can use the provided `python_stratified_split` function which holds out a percentage (in this case 25%) of items from each user, but ensures all users are in both `train` and `test` datasets. Other options are available in the `dataset.python_splitters` module which provide more control over how the split occurs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrGonprg2ULQ","executionInfo":{"status":"ok","timestamp":1702409129624,"user_tz":300,"elapsed":3754,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"ff7311c1-f46d-4d46-f5ea-8e26506f60fe"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.81k/4.81k [00:02<00:00, 2.32kKB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["   userID  itemID  rating  timestamp\n","0     196     242     3.0  881250949\n","1     186     302     3.0  891717742\n","2      22     377     1.0  878887116\n","3     244      51     2.0  880606923\n","4     166     346     1.0  886397596"],"text/html":["\n","  <div id=\"df-43543a56-c509-436f-9441-035a00fb508e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userID</th>\n","      <th>itemID</th>\n","      <th>rating</th>\n","      <th>timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>196</td>\n","      <td>242</td>\n","      <td>3.0</td>\n","      <td>881250949</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>186</td>\n","      <td>302</td>\n","      <td>3.0</td>\n","      <td>891717742</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22</td>\n","      <td>377</td>\n","      <td>1.0</td>\n","      <td>878887116</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>244</td>\n","      <td>51</td>\n","      <td>2.0</td>\n","      <td>880606923</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>166</td>\n","      <td>346</td>\n","      <td>1.0</td>\n","      <td>886397596</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43543a56-c509-436f-9441-035a00fb508e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-43543a56-c509-436f-9441-035a00fb508e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-43543a56-c509-436f-9441-035a00fb508e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-04070c7c-8c8f-424c-bc87-1a3deedadd67\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-04070c7c-8c8f-424c-bc87-1a3deedadd67')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-04070c7c-8c8f-424c-bc87-1a3deedadd67 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":42}],"source":["df = movielens.load_pandas_df(size=MOVIELENS_DATA_SIZE)\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AL80sQtR2ULS"},"outputs":[],"source":["train, test = python_stratified_split(df, ratio=0.75)"]},{"cell_type":"markdown","metadata":{"id":"nWEiQ6z02ULT"},"source":["### 2.2 Process data\n","\n","`ImplicitCF` is a class that intializes and loads data for the training process. During the initialization of this class, user IDs and item IDs are reindexed, ratings greater than zero are converted into implicit positive interaction, and adjacency matrix $R$ of user-item graph is created. Some important methods of `ImplicitCF` are:\n","\n","`get_norm_adj_mat`, load normalized adjacency matrix of user-item graph if it already exists in `adj_dir`, otherwise call `create_norm_adj_mat` to create the matrix and save the matrix if `adj_dir` is not `None`. This method will be called during the initialization process of LightGCN model.\n","\n","`create_norm_adj_mat`, create normalized adjacency matrix of user-item graph by calculating $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, where $\\mathbf{A}=\\left(\\begin{array}{cc}\\mathbf{0} & \\mathbf{R} \\\\ \\mathbf{R}^{T} & \\mathbf{0}\\end{array}\\right)$.\n","\n","`train_loader`, generate a batch of training data — sample a batch of users and then sample one positive item and one negative item for each user. This method will be called before each epoch of the training process.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyX5cU8f2ULU"},"outputs":[],"source":["data = ImplicitCF(train=train, test=test, seed=SEED)"]},{"cell_type":"code","source":["print(type(train))\n","print(train)\n","print(type(test))\n","print(test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lkrPszJqipaF","executionInfo":{"status":"ok","timestamp":1702414327172,"user_tz":300,"elapsed":143,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"outputId":"6ef276b6-ee96-496b-cf4c-be2061a9670b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","        userID  itemID  rating  timestamp\n","10           0      10     5.0  186821525\n","6            0       6     5.0  201163189\n","5            0       5     5.0  327608983\n","4            0       4     5.0  512083806\n","14           0      14     5.0  741271470\n","...        ...     ...     ...        ...\n","100302    1999    9438     5.0  968557062\n","100323    1999   10716     5.0  776019447\n","100309    1999    8730     5.0  956615188\n","100356    1999   10251     5.0  649685550\n","100364    1999     688     5.0  733286944\n","\n","[75324 rows x 4 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","        userID  itemID  rating  timestamp\n","12           0      12     5.0  667887525\n","7            0       7     5.0  567925303\n","1            0       1     5.0  193688187\n","11           0      11     5.0  601570397\n","70           1      70     5.0  158463543\n","...        ...     ...     ...        ...\n","100354    1999    1164     5.0  487976218\n","100340    1999    5870     5.0  901134757\n","100358    1999    1330     5.0  997155943\n","100355    1999    1084     5.0  951608633\n","100290    1999    6407     5.0  883776675\n","\n","[25046 rows x 4 columns]\n"]}]},{"cell_type":"markdown","metadata":{"id":"OQaOnp292ULU"},"source":["### 2.3 Prepare hyper-parameters\n","\n","Important parameters of `LightGCN` model are:\n","\n","`data`, initialized LightGCNDataset object.\n","\n","`epochs`, number of epochs for training.\n","\n","`n_layers`, number of layers of the model.\n","\n","`eval_epoch`, if it is not None, evaluation metrics will be calculated on test set every \"eval_epoch\" epochs. In this way, we can observe the effect of the model during the training process.\n","\n","`top_k`, the number of items to be recommended for each user when calculating ranking metrics.\n","\n","A complete list of parameters can be found in `yaml_file`. We use `prepare_hparams` to read the yaml file and prepare a full set of parameters for the model. Parameters passed as the function's parameters will overwrite yaml settings."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"xZcNhXE72ULU","executionInfo":{"status":"ok","timestamp":1702414332092,"user_tz":300,"elapsed":599,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}}},"outputs":[],"source":["hparams = prepare_hparams(yaml_file,\n","                          n_layers=3,\n","                          batch_size=BATCH_SIZE,\n","                          epochs=EPOCHS,\n","                          learning_rate=0.005,\n","                          eval_epoch=5,\n","                          top_k=TOP_K,\n","                         )"]},{"cell_type":"markdown","metadata":{"id":"hY54ebXe2ULU"},"source":["### 2.4 Create and train model\n","\n","With data and parameters prepared, we can create the LightGCN model.\n","\n","To train the model, we simply need to call the `fit()` method."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"9tHkGP182ULU","executionInfo":{"status":"ok","timestamp":1702414336429,"user_tz":300,"elapsed":2505,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a354c35b-1122-4435-8bc6-5cdf6bfb38f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Already create adjacency matrix.\n","Already normalize adjacency matrix.\n","Using xavier initialization.\n"]}],"source":["model = LightGCN(hparams, data, seed=SEED)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"xY-4a0662ULV","executionInfo":{"status":"ok","timestamp":1702414472578,"user_tz":300,"elapsed":135358,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb0bb0e8-e5da-410e-da0a-4113cab5d39a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 (train)2.6s: train loss = 0.42984 = (mf)0.42967 + (embed)0.00017\n","Epoch 2 (train)2.3s: train loss = 0.14379 = (mf)0.14317 + (embed)0.00062\n","Epoch 3 (train)3.4s: train loss = 0.12050 = (mf)0.11973 + (embed)0.00077\n","Epoch 4 (train)2.8s: train loss = 0.10885 = (mf)0.10799 + (embed)0.00086\n","Epoch 5 (train)2.4s + (eval)0.7s: train loss = 0.10075 = (mf)0.09981 + (embed)0.00094, recall = 0.04624, ndcg = 0.05769, precision = 0.04585, map = 0.01798\n","Epoch 6 (train)2.3s: train loss = 0.08997 = (mf)0.08896 + (embed)0.00102\n","Epoch 7 (train)2.2s: train loss = 0.08279 = (mf)0.08169 + (embed)0.00110\n","Epoch 8 (train)2.7s: train loss = 0.07872 = (mf)0.07754 + (embed)0.00117\n","Epoch 9 (train)2.8s: train loss = 0.07559 = (mf)0.07435 + (embed)0.00125\n","Epoch 10 (train)2.2s + (eval)0.6s: train loss = 0.06842 = (mf)0.06710 + (embed)0.00132, recall = 0.04788, ndcg = 0.05948, precision = 0.04765, map = 0.01864\n","Epoch 11 (train)2.4s: train loss = 0.06500 = (mf)0.06360 + (embed)0.00140\n","Epoch 12 (train)2.3s: train loss = 0.06436 = (mf)0.06289 + (embed)0.00147\n","Epoch 13 (train)2.5s: train loss = 0.05848 = (mf)0.05694 + (embed)0.00154\n","Epoch 14 (train)3.1s: train loss = 0.05581 = (mf)0.05420 + (embed)0.00161\n","Epoch 15 (train)2.5s + (eval)0.6s: train loss = 0.05319 = (mf)0.05151 + (embed)0.00167, recall = 0.05107, ndcg = 0.06396, precision = 0.04995, map = 0.02020\n","Epoch 16 (train)2.3s: train loss = 0.05219 = (mf)0.05045 + (embed)0.00174\n","Epoch 17 (train)2.3s: train loss = 0.05075 = (mf)0.04894 + (embed)0.00181\n","Epoch 18 (train)2.3s: train loss = 0.04818 = (mf)0.04631 + (embed)0.00187\n","Epoch 19 (train)3.0s: train loss = 0.04396 = (mf)0.04202 + (embed)0.00194\n","Epoch 20 (train)2.7s + (eval)0.7s: train loss = 0.04221 = (mf)0.04019 + (embed)0.00202, recall = 0.05283, ndcg = 0.06682, precision = 0.05205, map = 0.02139\n","Epoch 21 (train)2.4s: train loss = 0.04202 = (mf)0.03993 + (embed)0.00208\n","Epoch 22 (train)2.4s: train loss = 0.04039 = (mf)0.03824 + (embed)0.00215\n","Epoch 23 (train)2.3s: train loss = 0.03882 = (mf)0.03661 + (embed)0.00221\n","Epoch 24 (train)2.9s: train loss = 0.03597 = (mf)0.03370 + (embed)0.00227\n","Epoch 25 (train)2.8s + (eval)0.7s: train loss = 0.03594 = (mf)0.03360 + (embed)0.00234, recall = 0.05254, ndcg = 0.06684, precision = 0.05250, map = 0.02130\n","Epoch 26 (train)2.3s: train loss = 0.03284 = (mf)0.03042 + (embed)0.00241\n","Epoch 27 (train)2.4s: train loss = 0.03266 = (mf)0.03017 + (embed)0.00249\n","Epoch 28 (train)2.3s: train loss = 0.03216 = (mf)0.02962 + (embed)0.00254\n","Epoch 29 (train)2.9s: train loss = 0.02899 = (mf)0.02638 + (embed)0.00261\n","Epoch 30 (train)2.9s + (eval)0.7s: train loss = 0.02951 = (mf)0.02683 + (embed)0.00267, recall = 0.05150, ndcg = 0.06893, precision = 0.05235, map = 0.02209\n","Epoch 31 (train)2.3s: train loss = 0.02908 = (mf)0.02635 + (embed)0.00274\n","Epoch 32 (train)2.4s: train loss = 0.02647 = (mf)0.02367 + (embed)0.00280\n","Epoch 33 (train)2.4s: train loss = 0.02602 = (mf)0.02317 + (embed)0.00286\n","Epoch 34 (train)2.7s: train loss = 0.02651 = (mf)0.02359 + (embed)0.00292\n","Epoch 35 (train)3.0s + (eval)0.7s: train loss = 0.02526 = (mf)0.02229 + (embed)0.00297, recall = 0.05140, ndcg = 0.06794, precision = 0.05195, map = 0.02176\n","Epoch 36 (train)2.5s: train loss = 0.02368 = (mf)0.02065 + (embed)0.00302\n","Epoch 37 (train)2.5s: train loss = 0.02294 = (mf)0.01985 + (embed)0.00308\n","Epoch 38 (train)2.3s: train loss = 0.02311 = (mf)0.01997 + (embed)0.00314\n","Epoch 39 (train)2.8s: train loss = 0.02247 = (mf)0.01928 + (embed)0.00319\n","Epoch 40 (train)3.0s + (eval)0.7s: train loss = 0.02170 = (mf)0.01845 + (embed)0.00325, recall = 0.05306, ndcg = 0.06880, precision = 0.05275, map = 0.02197\n","Epoch 41 (train)2.3s: train loss = 0.02134 = (mf)0.01804 + (embed)0.00330\n","Epoch 42 (train)2.3s: train loss = 0.02087 = (mf)0.01752 + (embed)0.00335\n","Epoch 43 (train)2.3s: train loss = 0.01919 = (mf)0.01579 + (embed)0.00341\n","Epoch 44 (train)2.5s: train loss = 0.01842 = (mf)0.01496 + (embed)0.00346\n","Epoch 45 (train)3.1s + (eval)0.8s: train loss = 0.01934 = (mf)0.01583 + (embed)0.00351, recall = 0.05401, ndcg = 0.06930, precision = 0.05315, map = 0.02230\n","Epoch 46 (train)2.6s: train loss = 0.01836 = (mf)0.01479 + (embed)0.00356\n","Epoch 47 (train)2.4s: train loss = 0.01843 = (mf)0.01482 + (embed)0.00361\n","Epoch 48 (train)2.4s: train loss = 0.01802 = (mf)0.01436 + (embed)0.00366\n","Epoch 49 (train)2.6s: train loss = 0.01769 = (mf)0.01398 + (embed)0.00370\n","Epoch 50 (train)3.2s + (eval)0.7s: train loss = 0.01722 = (mf)0.01348 + (embed)0.00374, recall = 0.05639, ndcg = 0.07188, precision = 0.05425, map = 0.02341\n","Took 135.299018457 seconds for training.\n"]}],"source":["with Timer() as train_time:\n","    # model.fit(neg_mode=\"hard\") # hard, uniform, or ...\n","    # model.fit(neg_mode=\"epoch\", neg_size=500)\n","    model.fit(neg_mode=\"uniform\")\n","\n","print(\"Took {} seconds for training.\".format(train_time.interval))"]},{"cell_type":"code","source":["import numpy as np\n","\n","# Assuming filtered_scores is your matrix\n","a = np.random.random((10, 928))  # Replace this with your actual matrix\n","\n","# Find the row index with the maximum value in the entire matrix\n","\n","indices = np.where(a == a.max())\n","print(indices)\n"],"metadata":{"id":"qhKkdq8kI4XL","executionInfo":{"status":"ok","timestamp":1702409373260,"user_tz":300,"elapsed":18,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca2984e8-b964-4899-e2a7-93416d21222f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(array([0]), array([531]))\n"]}]},{"cell_type":"code","source":["print(model.n_users)\n","print(model.score(np.array([940])))"],"metadata":{"id":"ggx0HLq6FvkP","executionInfo":{"status":"ok","timestamp":1702409373260,"user_tz":300,"elapsed":17,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e221b5e0-b04a-4435-8261-8b99377a4c4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["943\n","[[ 2.5960646   1.618256    0.16865893 ... -0.94321936 -0.7642796\n","  -0.8080341 ]]\n"]}]},{"cell_type":"code","source":["print(len(df['userID'].unique()))\n","print(len(df['itemID'].unique()))"],"metadata":{"id":"vKs2NZllTSX9","executionInfo":{"status":"ok","timestamp":1702409373260,"user_tz":300,"elapsed":16,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca5a79c2-4504-4cfa-9a9a-ff4007940368"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["943\n","1682\n"]}]},{"cell_type":"code","source":["# for userid in df['userID'].unique():\n","#     print(model.score(np.array([userid])), model.score(np.array([userid])).shape)\n","\n"],"metadata":{"id":"aHyfWEIJNJ2o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IcfbVaNe2ULV"},"source":["### 2.5 Recommendation and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"vXwZKdS62ULV"},"source":["Recommendation and evaluation have been performed on the specified test set during training. After training, we can also use the model to perform recommendation and evalution on other data. Here we still use `test` as test data, but `test` can be replaced by other data with similar data structure."]},{"cell_type":"markdown","metadata":{"id":"XN0MMDwl2ULV"},"source":["#### 2.5.1 Recommendation\n","\n","We can call `recommend_k_items` to recommend k items for each user passed in this function. We set `remove_seen=True` to remove the items already seen by the user. The function returns a dataframe, containing each user and top k items recommended to them and the corresponding ranking scores."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"iSjLEltQ2ULV","executionInfo":{"status":"ok","timestamp":1702414496832,"user_tz":300,"elapsed":795,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"3dc2ad59-954c-416d-d8a0-78977e4d15d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   userID  itemID  prediction\n","0       0     649    8.544640\n","1       0     762    8.352934\n","2       0      11    8.090770\n","3       0     652    7.765273\n","4       0     807    7.685188"],"text/html":["\n","  <div id=\"df-5f7f7005-f79c-414f-8937-0fc6ee70f349\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userID</th>\n","      <th>itemID</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>649</td>\n","      <td>8.544640</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>762</td>\n","      <td>8.352934</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>8.090770</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>652</td>\n","      <td>7.765273</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>807</td>\n","      <td>7.685188</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f7f7005-f79c-414f-8937-0fc6ee70f349')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5f7f7005-f79c-414f-8937-0fc6ee70f349 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5f7f7005-f79c-414f-8937-0fc6ee70f349');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-046d4945-32a1-401c-a7bf-b0ba49663109\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-046d4945-32a1-401c-a7bf-b0ba49663109')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-046d4945-32a1-401c-a7bf-b0ba49663109 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":19}],"source":["topk_scores = model.recommend_k_items(test, top_k=TOP_K, remove_seen=True)\n","\n","topk_scores.head()"]},{"cell_type":"markdown","metadata":{"id":"LGiwj1yt2ULV"},"source":["#### 2.5.2 Evaluation\n","\n","With `topk_scores` predicted by the model, we can evaluate how LightGCN performs on this test set."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"xhEFuysI2ULW","executionInfo":{"status":"ok","timestamp":1702414497799,"user_tz":300,"elapsed":312,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"24d426a4-864e-42bc-901f-779622c15464"},"outputs":[{"output_type":"stream","name":"stdout","text":["MAP:\t0.023405\n","NDCG:\t0.071884\n","Precision@K:\t0.054250\n","Recall@K:\t0.056386\n"]}],"source":["eval_map = map_at_k(test, topk_scores, k=TOP_K)\n","eval_ndcg = ndcg_at_k(test, topk_scores, k=TOP_K)\n","eval_precision = precision_at_k(test, topk_scores, k=TOP_K)\n","eval_recall = recall_at_k(test, topk_scores, k=TOP_K)\n","\n","print(\"MAP:\\t%f\" % eval_map,\n","      \"NDCG:\\t%f\" % eval_ndcg,\n","      \"Precision@K:\\t%f\" % eval_precision,\n","      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"]},{"cell_type":"code","execution_count":41,"metadata":{"scrolled":true,"id":"sNi3ahPF2ULW","executionInfo":{"status":"ok","timestamp":1702414213557,"user_tz":300,"elapsed":325,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"3393a5f0-3249-4110-a27e-4b00e9255258"},"outputs":[{"output_type":"display_data","data":{"application/scrapbook.scrap.json+json":{"name":"map","data":0.011659972548211531,"encoder":"json","version":1}},"metadata":{"scrapbook":{"name":"map","data":true,"display":false}}},{"output_type":"display_data","data":{"application/scrapbook.scrap.json+json":{"name":"ndcg","data":0.05363145374808597,"encoder":"json","version":1}},"metadata":{"scrapbook":{"name":"ndcg","data":true,"display":false}}},{"output_type":"display_data","data":{"application/scrapbook.scrap.json+json":{"name":"precision","data":0.045399999999999996,"encoder":"json","version":1}},"metadata":{"scrapbook":{"name":"precision","data":true,"display":false}}},{"output_type":"display_data","data":{"application/scrapbook.scrap.json+json":{"name":"recall","data":0.026088876493318466,"encoder":"json","version":1}},"metadata":{"scrapbook":{"name":"recall","data":true,"display":false}}}],"source":["# Record results with papermill for tests\n","sb.glue(\"map\", eval_map)\n","sb.glue(\"ndcg\", eval_ndcg)\n","sb.glue(\"precision\", eval_precision)\n","sb.glue(\"recall\", eval_recall)"]},{"cell_type":"markdown","metadata":{"id":"WvyCGlhk2ULW"},"source":["### 2.6 Infer embeddings\n","\n","With `infer_embedding` method of LightGCN model, we can export the embeddings of users and items in the training set to CSV files for future use."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"Jg7tM0rH2ULX","executionInfo":{"status":"ok","timestamp":1702414219496,"user_tz":300,"elapsed":4036,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}}},"outputs":[],"source":["model.infer_embedding(user_file, item_file)"]},{"cell_type":"markdown","metadata":{"id":"dHTCISLP2ULX"},"source":["## 3. Compare LightGCN with SAR and NCF\n","\n","Here there are the performances of LightGCN compared to [SAR](../00_quick_start/sar_movielens.ipynb) and [NCF](../00_quick_start/ncf_movielens.ipynb) on MovieLens dataset of 100k and 1m. The method of data loading and splitting is the same as that described above and the GPU used was a GeForce GTX 1080Ti.\n","\n","Settings common to the three models: `epochs=15, seed=42`.\n","\n","Settings for LightGCN: `embed_size=64, n_layers=3, batch_size=1024, decay=0.0001, learning_rate=0.015 `.\n","\n","Settings for SAR: `similarity_type=\"jaccard\", time_decay_coefficient=30, time_now=None, timedecay_formula=True`.\n","\n","Settings for NCF: `n_factors=4, layer_sizes=[16, 8, 4], batch_size=1024, learning_rate=0.001`.\n","\n","| Data Size | Model    | Training time | Recommending time | MAP@10   | nDCG@10  | Precision@10 | Recall@10 |\n","| --------- | -------- | ------------- | ----------------- | -------- | -------- | ------------ | --------- |\n","| 100k      | LightGCN | 27.8865       | 0.6445            | 0.129236 | 0.436297 | 0.381866     | 0.205816  |\n","| 100k      | SAR      | 0.4895        | 0.1144            | 0.110591 | 0.382461 | 0.330753     | 0.176385  |\n","| 100k      | NCF      | 116.3174      | 7.7660            | 0.105725 | 0.387603 | 0.342100     | 0.174580  |\n","| 1m        | LightGCN | 396.7298      | 1.4343            | 0.075012 | 0.377501 | 0.345679     | 0.128096  |\n","| 1m        | SAR      | 4.5593        | 2.8357            | 0.060579 | 0.299245 | 0.270116     | 0.104350  |\n","| 1m        | NCF      | 1601.5846     | 85.4567           | 0.062821 | 0.348770 | 0.320613     | 0.108121  |\n","\n","From the above results, we can see that LightGCN performs better than the other two models."]},{"cell_type":"markdown","metadata":{"id":"WqwQSWDG2ULX"},"source":["### References:\n","1. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang & Meng Wang, LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, 2020, https://arxiv.org/abs/2002.02126\n","2. LightGCN implementation [TensorFlow]: https://github.com/kuandeng/lightgcn\n","3. Thomas N. Kipf and Max Welling, Semi-Supervised Classification with Graph Convolutional Networks, ICLR, 2017, https://arxiv.org/abs/1609.02907\n","4. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua, Neural Graph Collaborative Filtering, SIGIR, 2019, https://arxiv.org/abs/1905.08108\n","5. Y. Koren, R. Bell and C. Volinsky, \"Matrix Factorization Techniques for Recommender Systems\", in Computer, vol. 42, no. 8, pp. 30-37, Aug. 2009, doi: 10.1109/MC.2009.263.  url: https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf"]},{"cell_type":"markdown","source":["### Amazon Book Dataset"],"metadata":{"id":"BPM-8x5QvaEQ"}},{"cell_type":"code","source":["!ls recommenders/datasets/amazon-book/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PiSBz5tFqMOK","executionInfo":{"status":"ok","timestamp":1702412120301,"user_tz":300,"elapsed":311,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"outputId":"cb9d8eff-cd61-4e7d-d7ab-6d1e9e0fc9bc"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["item_list.txt  test.txt  train.txt  user_list.txt\n"]}]},{"cell_type":"code","source":["import random\n","import pandas as pd\n","\n","path = \"recommenders/datasets/amazon-book/\"\n","train_file = path + '/train.txt'\n","data = []\n","\n","\n","with open(train_file) as f:\n","    for l in f.readlines()[:1000]: # take first 1000 users\n","        if len(l) > 0:\n","            l = l.strip('\\n').split(' ')\n","            items = [int(i) for i in l[1:]]\n","            uid = int(l[0])\n","            for item_id in items:\n","                timestamp = random.randint(100000000, 999999999)\n","                data.append([uid, item_id, 5.0, timestamp])\n","\n","df = pd.DataFrame(data, columns=['userID', 'itemID', 'rating', 'timestamp'])\n","train, test = python_stratified_split(df, ratio=0.75)\n","print(train.head)\n","print(test.head)\n","data = ImplicitCF(train=train, test=test, seed=SEED)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GFyRpzE3q2bc","executionInfo":{"status":"ok","timestamp":1702414003096,"user_tz":300,"elapsed":669,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"outputId":"7e68ffd0-24e6-4236-a38f-31e0451cebfd"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method NDFrame.head of         userID  itemID  rating  timestamp\n","10           0      10     5.0  826600539\n","42           0      42     5.0  465341213\n","29           0      29     5.0  685126461\n","6            0       6     5.0  339670711\n","32           0      32     5.0  582334538\n","...        ...     ...     ...        ...\n","109889     999   37578     5.0  307211065\n","109925     999   29219     5.0  467502268\n","109892     999   30066     5.0  583971219\n","109914     999    5780     5.0  826658850\n","109909     999    4598     5.0  347087564\n","\n","[82451 rows x 4 columns]>\n","<bound method NDFrame.head of         userID  itemID  rating  timestamp\n","2            0       2     5.0  126855092\n","51           0      51     5.0  203848421\n","25           0      25     5.0  313500298\n","35           0      35     5.0  969119330\n","12           0      12     5.0  685582861\n","...        ...     ...     ...        ...\n","109922     999    7259     5.0  231030723\n","109910     999   37586     5.0  548583787\n","109876     999    2006     5.0  810303017\n","109890     999   32920     5.0  840066310\n","109899     999    2158     5.0  289655371\n","\n","[27479 rows x 4 columns]>\n"]}]},{"cell_type":"markdown","source":["### Yelp2018 Dataset"],"metadata":{"id":"inVnfHaBxUJz"}},{"cell_type":"code","source":["!ls recommenders/datasets/yelp2018/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83e_OmoKxW5D","executionInfo":{"status":"ok","timestamp":1702414308119,"user_tz":300,"elapsed":152,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"outputId":"2db4f33c-1ca5-499d-b6f8-29ef1c3be4cf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["train.txt\n"]}]},{"cell_type":"code","source":["import random\n","import pandas as pd\n","\n","path = \"recommenders/datasets/yelp2018/\"\n","train_file = path + '/train.txt'\n","data = []\n","\n","\n","with open(train_file) as f:\n","    for l in f.readlines()[:2000]: # take first 2000 users\n","        if len(l) > 0:\n","            l = l.strip('\\n').split(' ')\n","            items = [int(i) for i in l[1:]]\n","            uid = int(l[0])\n","            for item_id in items:\n","                timestamp = random.randint(100000000, 999999999)\n","                data.append([uid, item_id, 5.0, timestamp])\n","\n","df = pd.DataFrame(data, columns=['userID', 'itemID', 'rating', 'timestamp'])\n","train, test = python_stratified_split(df, ratio=0.75)\n","print(train.head)\n","print(test.head)\n","data = ImplicitCF(train=train, test=test, seed=SEED)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UH1SQRamxaOF","executionInfo":{"status":"ok","timestamp":1702414311765,"user_tz":300,"elapsed":2990,"user":{"displayName":"Yining Wang","userId":"06946062052070440642"}},"outputId":"1bccfacf-b979-4cba-defd-48fc89e15afb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method NDFrame.head of         userID  itemID  rating  timestamp\n","10           0      10     5.0  186821525\n","6            0       6     5.0  201163189\n","5            0       5     5.0  327608983\n","4            0       4     5.0  512083806\n","14           0      14     5.0  741271470\n","...        ...     ...     ...        ...\n","100302    1999    9438     5.0  968557062\n","100323    1999   10716     5.0  776019447\n","100309    1999    8730     5.0  956615188\n","100356    1999   10251     5.0  649685550\n","100364    1999     688     5.0  733286944\n","\n","[75324 rows x 4 columns]>\n","<bound method NDFrame.head of         userID  itemID  rating  timestamp\n","12           0      12     5.0  667887525\n","7            0       7     5.0  567925303\n","1            0       1     5.0  193688187\n","11           0      11     5.0  601570397\n","70           1      70     5.0  158463543\n","...        ...     ...     ...        ...\n","100354    1999    1164     5.0  487976218\n","100340    1999    5870     5.0  901134757\n","100358    1999    1330     5.0  997155943\n","100355    1999    1084     5.0  951608633\n","100290    1999    6407     5.0  883776675\n","\n","[25046 rows x 4 columns]>\n"]}]}],"metadata":{"celltoolbar":"Tags","interpreter":{"hash":"3a9a0c422ff9f08d62211b9648017c63b0a26d2c935edc37ebb8453675d13bb5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}